#fsdb -F t N	Title	Authors_with_affiliations	Email	Keywords	Area/Topic	Decision_deadline	Reviewer	Reviewers_Suggestion	Finalish_Decision	Scheduled_Slot_Discussion	Decision_letter_sent	Feedback_sent	Comments_to_authors	TPC_comments	Evgeny's_Comments	Proposed_Sessions
1	A Case for Long-Term Statistics  camera-ready: https://docs.google.com/document/d/1j3Wkwqyemoz-ISO1W9I_kzxP4LhVLu74/edit?usp=sharing&ouid=111226047458311420927&rtpof=true&sd=true	Michael Welzl	michawe@ifi.uio.no			8/16/2021	Jari	Jari: In but perhaps borderline. On topic. The main point of the paper is well-founded and central to the workshop. Albeit perhaps a bit simple too, the paper could have provided a more in-depth discussion — e.g., of where it applies or what issues a more long-term measurement model would imply — but did not. It would be nice to discuss what would be needed for an application, host os, home router, access network, transit network, cloud platform,  etc. to maintain more long-term measurements, or how one could access them from measurement tool or applications. I also found the examples mixed a few things, the long-term measurement angle vs. several components contributing to capacity/problems, and the nature of the specific application flows.	In	Day 1	Yes	Review			Some intro to the usefulness of the workshop. Too general ideas	Intro
2	The Futility of QoS 	Yaakov (J) Stein 	author@dspcsp.com, yaakov_s@rad.com			8/16/2021	Toke Greg Geoff	Greg: This was an entertaining read.  The author's main conclusion is that mapping QoS to QoE is beyond hard, and so the only thing that matters is direct QoE measurement. It is an interesting and possibly controversial conclusion, but I'm left wondering if it is helpful in the context of the workshop.  I would say "maybe".   Geoff: I suggest  yes as I think it is a useful review of the evolution of the QoS thinking over the past 25 years or so! Toke: I wasn't overly impressed with the QoS review. I do think there's a certain reason to the overall point that measuring the actual application performance is the right thing to do instead of using QoS as a stand-in, but like Greg I'm not sure if it's going to be terribly relevant to the workshop. So I guess that's another maybe from me.	In	Day 1	Yes	Review			General intro	Intro
3	State of Wi-Fi Reporting	Ken Kerpez, Jinous Shafiei, John Cioffi, Pete Chow, Djamel Bousaber  ASSIA inc	kkerpez@assia-inc.com, jshafiei@assia-inc.com, jcioffi@assia-inc.com, pchow@assia-inc.com, dbousaber@assia-inc.com	QoE, Wireless		8/16/2021	Evgeny	The authors present report on the Wi-Fi performance. The report includes data for more than 6 months of estimations in both North America and Europe. The authors estimate different KPIs including amount of DL and UL traffic, latency, interference, congestion, and the ration of achieved throughput to MCS data rate. They show that the amount of traffic and the interference in both 2.4 GHz and 5 GHz are continiously increasing, and we will need to use 6 GHz spectrum to satisfy QoE in near future as shows Spectrum-Need Score (SNS) metric proposed by authors. Also the authors provide results showing that QoE is mostly limited by Wi-Fi, but not by broadband (FTTH). Accept 	In	Day 2	Yes	Sent		Organize a session on QoE in wireless internet	QoE in Wireless	Wireless
4	Objective and subjective network quality	Joachim Fabini	joachim.fabini@tuwien.ac.at	QoE, Visualization		8/16/2021	Toke Tommy	Tommy: Include. Joachim is mainly pointing to RFC 7312, which he co-authored, as the right framework to use for measurement. He is taking an interesting stance that it isn't possible to condense quality into a single userful metric across all regions / societies, due to the differing usage patterns. This kind of skepticism is useful, coming from a background with experience in measurement. Toke: Include. The "it's complicated" point is good to keep in mind, and having that input (backed by experience) is likely to be worthwhile in the workshop. I also think it would be interesting to explore the idea of a multi-faceted "visualisation" of different aspects of performance that Joachim suggests at the end of his position paper.	In	Day 1	Yes	Sent			Difficulty in QoE Visualization	Metrics
5	Network Quality and impact on Transport Performance for  End-Users	Gorry Fairhurst	gorry@erg.abdn.ac.uk	transport protocol design, performance optimization, path support		8/16/2021	Toke, Tommy, Mirja	Tommy: Maybe. Mainly discussing how transport protocol determines experienced network quality, and arguing that using measurements during the design of transport protocols can further benefit them. There are some good references to features supported by networks that would be useful to measure or identify. The paper needs some cleanup for typos, and would benefit from revision. Mirja: Recommend to invite. The paper discusses the need of measurement data for protocol design as well as the need to measure path properties in order to selected/configure the right protocol and path. I think in general the interaction of measurement data and availablity of meassurements in order to optimize protocol performance and design is an interesting aspect to discuss at the workshop. Toke: Include. The point about transport protocols needing to be based on measurements of actual (and diverse) path characteristics is well-made, and it's an aspect that is too often overlooked in protocol design. Having that be part of the discussion at the workshop seems worthwhile.	in		Yes	Review			Heterogeneous networks and cross-layer 	Cross-layer
6	Access Network Quality as Fitness for Purpose	Kenjiro Cho	kjc@iijlab.net	measurement		8/16/2021	Toke, Olivier	Toke: Maybe. The paper nicely outlines the deficiencies of the classic speedtest, and nicely outlines why it's not fit for purpose. However, the proposed "fitness for purpose" metric to put in its stead seems a bit simplistic. The idea that it should be possible to define a "standard use" for an internet connection and have a binary "works/doesn't work" for that use case seems awfully simplistic and I have a hard time seeing how such a scheme would solve anything.Olivier: Kenjiro brings an interesting point in the debate, the too strong reliance on speedtest. In section 4, he argues for a different type of test that would simply verify whether the Internet access satisfies given criteria for x persons doing a given type of work. He also argues about the energy impact of the speedtest which is relevant as well. He has a lot experience in Internet measurements and his insights would be very useful. 	In	D	Yes	Review			Disadvantages of speedtest	Metrics
7	In Search of Lost QoS	Kalevi Kilkki, Benajamin Finley	kalevi.kilkki@aalto.fi	QoS, provisioning		8/16/2021	Jari Geoff	Geoff: In. I particularly appreciate the broad perspective in this paper and the indirect nod to the work of Andrew Odlyzko how has maintained for many years that the cheapest pragmatic answer to selective rationing of network resources was simply to overprovision. Jari: In. Wow, quite a paper (35 pages), very much in-depth of the full field and history, and reasons for failures. Talks about their idea of incentive-based QoS in the end. Well worth the read. The authors would be really good discussion partners in the workshop, even if the paper's topic is not exactly on our topic...	In	Day 3	Yes	Review			Ways to provide QoS	Solutions
8	Measuring & Improving QoE on the Xfinity Wi-Fi Network   Camera-ready https://drive.google.com/file/d/1LrYJF7IaVXGw_JnOfmPM61lPmYS14P5c/view?usp=sharing	Rajat Ghai	Rajat_Ghai=40comcast.com@dmarc.ietf.org	QoS, wireless		8/16/2021	Evgeny Jari	Evgeny: The paper is devoted to the QoE estimation for Wi-Fi in Comcast which is the largest ISP in the United States. The authors propose to passively evaluate QoE by estimating TCP connection latency (time between generation of SYN+ACK and ACK reception), number of TCP retransmissions, duration of connection to the network, and throughput. They consider that QoE is acceptable if measured KPI is below predefined threshold (e.g. 150ms for TCP connection latency, 4% for TCP ReTXs). However, the authors do not provide any results to prove that the proposed approach is efficient, or to justify the used threshold values. Likely Accept	In	Day 2	Yes	Sent		I believe the email should be Rajat_Ghai@comcast.com	Ways to measure QoS in Wireless	Wireless
9	Quality Attenuation as a framework for measuring and evaluating end-user network experiences   Camera-ready: https://drive.google.com/file/d/14AQB279laDd5Ss8ucH8fOuKXm2tAPMMl/view?usp=sharing 	Neil Davies, Peter Thompson	neil.davies@pnsol.com, peter.thompson@pnsol.com	QoS, provisioning, framework, solution		8/16/2021	Evgeny Jason	Jason: I am familiar with "delta-Q" and even tried to implement it (was unable to). The concept is interesting but seems overtaken to some extent by development of latency under load measurements. IMO it is a "maybe" so I defer to Evgeny and other reviewers. In full disclosure of COI - when I tried to implement delta-Q it was via a consulting agreement with the authors and was at least 5 years ago if not 8 or 9 years. I am aware my prior experience may negatively color my review so I wanted to be clear on the prior history and leave it to another reviewer. I would not oppose inclusion.  Evgeny: The paper and the idea of the authors' proposal fit well for the topic of the workshop. Although the main idea is not clearly stated, I think that these ideas are worth discussing provided that the authors could clearly and briefly summarize their proposal. Right now it is a borderline submission. But the cited documents give evidence that the huge work has been done, and the members of Broadband form have put much efforts into this methodology. 	In (panel discussion only, no presentation)		Yes	Sent			Ways to provide QoS	Solutions
10	Measuring Network Experience Meaningfully, Accurately, and Scalably	Vijay Sivaraman, Sharat Madanapalli, Himal Kumar	vijay@unsw.edu.au	QoE, ML, measurement		8/16/2021	Mirja	Mirja: Recommend to invite. This paper argues that metric for user should focus on application layer metrics, such as video freezes or game lag, and they discuss use of ML to detrive application layer metric from network traffic observations. This is an interesting perspective and in-scope.	In	Day 2	Yes	Sent			Ways to measure QoS	Metrics
11	Focusing on latency, not throughput, to provide better internet experience and network quality	Gino Dion 	gino.dion@nokia.com	QoS, wireless, latency		8/16/2021	Sam	Sam: Recommend to invite. This paper begins by stating that we should not strive for low latency, but *consistent* latency (I don't think this is particularly shocking to anyone here). The author presents an example where they found and fixed a latency volatility issue, but - to their credit - do not claim that this can be used in the general case. Instead they argue that if we want to solve the problem of bufferbloat then we cannot do so within a single domain alone (e.g. an improved queuing algorithm), we need the owners of all parts of the stack to work together cohesively. They correctly identify the major parts of the stack that would need to work together (SoC vendors, network vendors, operators, app providers, etc). I think their depth of knowledge would be a valuable addition to the workshop.	In	Day 2	Yes	Sent			Importance of the latency, QoS in Wireless	Wireless or metrics
12	Challenges and opportunities of hardware support for Low Queueing Latency without Packet Loss	Koen De Schepper, Olivier Tilmans, Gino Dion	koen.de_schepper@nokia-bell-labs.com, olivier.tilmans@nokia-bell-labs.com, gino.dion@nokia.com	aggregation quantum, hardware delays, cross-layer interation		8/16/2021	Evgeny Jason	Jason suggests inclusion Evgeny: The paper states that the best QoE is achieved when the expectations of the applications (or user) meet with the QoS that the network provides. This way, to achieve the best QoE, the network shall provide small and predictable latencies (i.e., little congestion). That's why the paper addresses the hardware-related issues that need to be solved to reduce congestion in the networks. The submission addresses the issue of packet aggregation, cross-layer interaction and packet service in modern [wireless] networks to improve QoE. Accept.	In	Day 2	Yes	Sent			QoS in Wireless	Wireless
13	Measuring ISP Performance in Broadband America: a Study of Latency Under Load	Dave Reed, Levi Perigo	david.reed@colorado.edu, Levi.Perigo@Colorado.EDU	latency-under-load (LUL), measurement data, FCC, latency, load		8/16/2021	Mirja	Mirja: Recommend to invite. The paper shows an anylsis of latency-under-load (LUL) data from the FCC's Measuring Broadband America program for three ISP using different access technologies. There is certainly even more analysis that could be done, but I think this is interesting inpt data for the discussion at the workshop. One comment for the authors: If I understood correctly you show the "raw" RTT data. It would probably be more interesting to substract the base/minimum measured RTT and only show the additional delay under load, as usually every user/probe probably has a different base RTT to the measured server, right? 	In	Day 2	Yes	Sent			Latency under load	Metrics
14	Incentive-Based Traffic Management and QoS Measurements	Sandor Laki, Szilveszter Nadas, Balazs Varga, Luis M. Contreras	lakis@inf.elte.hu, szilveszter.nadas@ericsson.com, balazs.a.varga@ericsson.com, luismiguel.contrerasmurillo@telefonica.com	QoS, wireless, strategy, solution		8/16/2021	Jari, Evgeny, Olivier	Evgeny: The paper considers paradigms how QoS canbe satified at different levels. I consider the topic important for the workshop. Accept. Olivier: The paper presents a broad discussion of QoS and related topics but remains very generic. The future work section encourages further research on incentive based traffic management, but does not discuss how these could be deployed.	In	Day 3	Yes	Sent		Mirja has COI	Cross-layer, QoS in Wireless	Cross-layer or wireless
15	Dream-Pipe or Pipe-Dream: What Do Users Want (and how can we assure it)?	Al Morton	acmorton@att.com	measurement		8/16/2021	Jari	Jari: In. Paper starts with a discussion of the differing needs and expectations different people may have on their "pipe". It then goes on to discuss the history of the work on metric definitions at the IETF and elsewhere. There's also a discussion how that relates to user-perceivable experiences. It suggests the need for debugging capabilities (where is the problem), the use of derived metrics (metrics that can better explain something), and categorization of metrics rather than detailed work on individual metrics. 	In	Day 1	Yes	Sent			Existing Metrics	Metrics
16	Observability is needed to improve network quality	Jari Arkko, Mirja Kuehlewind	jari.arkko@piuha.net, ietf@kuehlewind.net	in-network measurement, secure measurement, protocol design for measurement		8/16/2021	Kathie	Kathie: Accept. This shows the authors’ knowledge of the state-of-the-art in measurement and makes the points that 1) observability and built-in measurement capabilities should be considered in the design of protocols and networks and 2) standardized definitions of measurement data and secure request and exchange of these measurements is needed. These are important issues, though difficult to achieve in wide distribution. I take some issue with the second inclusion from the IAB Covid-19 workshop about applications hiding a lost of loss to ensure a good user experience. Though this might be difficult in terms of diagnosis of network problems, I think for this workshop on network quality for end-users, it is a non-issue since end-users quality is not affected.	In	Day 2	Yes	Sent			Cross-layer cooperation	Cross-layer 
17	Metrics helpful in assessing Internet Quality	Jonathan Foulkes	jfoulkes@evenroute.com	latency, measurement		8/16/2021	Jason, Olivier	Include. Very interesting real-world data and experience with working latency reduction. Olivier: the paper discusses several intersting metrics collected from real users, but this is mainly presented as a series of anecdotes. Itwould be useful to try to have a broader view and better organisation the presentation.	In	Day 2	Yes	Sent			Metrics	Metrics
18	Preliminary Longitudinal Study of Internet Responsiveness	Matt Mathis	mattmathis=40google.com@dmarc.ietf.org	measurement, experimentation		8/16/2021	Jari, Olivier	Jari: In. This on-topic paper describes work around Measurement Lab and on "Internet responsiveness", defined as focusing on roundtrips. The Roundtrips Per Minute (RPM) metric is recently specified, but has already been use in some commercial implementations much earlier.  Measurement data from NYC and US are presented.  The paper concludes that much work remains to be done, after listing a number of issues making these measurements difficult to do in practice.Olivier: Work in progress paper, but presents  an interesting metric and collects it over a decade using MLAB. The results need to be a bti cleaned before being presented and understood, but there is a clear value in this paper.	In	Day 2	Yes	Sent			Metrics	Metrics
19	Cross-layer Cooperation for Better Network Service    camera-ready: https://drive.google.com/file/d/1qSk9s3_uv55_Q8sSTPTlJorTgdYbei6e/view?usp=sharing	Mikhail Liubogoshchev	liubogoshchev@wireless.iitp.ru	live adaptation, buffering, aggregation, apn negotiation, QoS in Wireless		8/16/2021	Tommy	Tommy: Maybe, if room. The paper is short and not particularly well-written. The suggestion it makes, to have more communication between applications and network devices, is not very well fleshed-out, but would be a topic worth discussing. However, it isn't clear that relying on networks explaining their quality (as opposed to clients measuring quality) would be fruitful without more specifics.	In (panel discussion only, no presentation)	Discussion only	Yes	Review		Evgeny has COI	Cross-layer, Wireless	Cross-layer or wireless
20	Merge Those Metrics: Towards Holistic (Protocol) Logging	Robin Marx, Joris Herbots	robin.marx@kuleuven.be, joris.herbots@uhasselt.be	Cross-layer/cross-location telemetry/logging, data sharing, solution,  measurement		8/16/2021	Jari, Mirja	Jari: In. This on-topic paper suggests that observations and logging from traffic be done in an integrated fashion, both from cross-layer and cross-vantage point perspective. A good point, and well justified in the paper. Including an extensive discussion of challenges. Worth a read! Nits: on page 2, the paper groups together spin and generic UDP path layer approaches, and suggests privacy concerns are the primary reason for not adopting either. I think those two approaches are very different. Also, given the privacy analysis of spin bit, there's little reason to believe those are the reasons (particularly when the same parties share terabytes of detailed data about users through other means). More likely, the reasons relate to lack of benefit from endpoint side. It is also a changing field. On page 3, the paper talks about the difficulties in collecting data from multiple vantage points, mainly from the perspective of the necessary log schemas. I'd add that there seems to be deeper protocol issues as well, e.g., QUIC isn't particulary easy protocol to track connection identifiers given that their length field is not included in the packets. You can do that, but at a cost. Mirja: Accept. This well-written papers discussess the need and challenges for cross-layer as well as cross-location logging based on a common log format. It also discusses the need for sharing data in order to get a more realistic few on questions this workshop raised. Both points are in scope and sshould be discusses at the workshop IMO.	In	Day 2	Yes	Sent			Cross-layer, metrics	Solutions
21	Error Performance Measurement in Packet-Switched Networks	Gregory Mirsky, Xiao Min, Gyan Mishra, Liuyan Han	gregimirsky@gmail.com, xiao.min2@zte.com.cn, gyan.s.mishra@verizon.com, hanliuyan@chinamobile.com	measurements, standards, error-rate		8/16/2021	Tommy	Tommy: Recommend inclusion. This entry is mainly pointing to existing Internet Drafts/RFCs in IPPM and elsewhere. Particularly, the authors discuss error-rate measurement as a common technique on constant-rate links, and discuss how such a measurement would work on packet-switched networks. This measurement of quality based on error-rate as opposed to strictly latency or throughput is a different approach than many of others I've seen here, and would be worth including.	In	Day 2	Yes	Sent			Metrics measurements 	Metrics
22	Transport Layer Statistics for Network Quality	Praveen Balasubramanian	pravb=40microsoft.com@dmarc.ietf.org	transport and application measurement		8/16/2021	Sam	Sam: Recommend inclusion if room. The author makes the case for capturing and correlating transport layer metrics alongside application layer metrics (with the goal of using the former to understand issues with the latter, or rule out the network at least). In my own experience, these are too often treated as independent things - run by separate teams, monitored separately, with no direct way to correlate between them apples-for-apples. This can lead to investigations going down the wrong path, or changes in one place having no accountability in metrics in another. I'm aware of at least one large tech company that already does this in its consumer facing app, so clearly others think this is useful too.	In	D	Yes	Sent			Metrics measurements 	Metrics
23	The Internet Exists In Its Use	Jana Iyengar	jri=40fastly.com@dmarc.ietf.org	differential QoE, application-based metrics, use-based QoE		8/16/2021	Kathie, Jari, Mirja	Kathie: Invite participation: This is very much a position and not a technical contribution. Since I know something about the author, I am assured this comes from a technical place. What I find compelling here is 1) measuring QoE is very much dependent on usage and, in particular, first world QoE might be different from third world QoE or QoE where there is state-controlled Internet access and 2) there can’t be a single QoE metric and metrics need to evolve.  Jari: In. Jana makes a short but valid point that the type of usage and applications determine what counts as good result, and any metric work should respect that. Mirja: This paper makes one focused point that metrics need to evolve depending the Internet use. This is a very important observation but maybe a bit of a side track for this workshop, as we really want too discuss how metric need to evolve/change right now. I think discussing way to more dynamically adopt metric in future (without hloding another IAB workshop) is also a good discussion topic, however, I'd wish the paper would provide more ideas how to achieve that rather than just this statement.  	In	Day 1	Yes	Yes			Metrics	Intro
24	Five Observations on Measuring Network Quality for Users of Real-Time Media Applications	Keith Winstein	keithw@cs.stanford.edu	latency-under-load (LUL), application layer quality, signal delay		8/16/2021	Jason, Mirja 	Maybe - some interesting observations that appear to lead to a conclusion that RTC apps need app-layer QoE measurement standards or that this would be useful. Would be good to have another person review. Mirja: also maybe. The message of this paper is not fully clear to me. I believe the main take away is that tradtional network based metrics are not enough as the relation to application quality is "complicated" and latency-under-load (LUL) as well as "signal delay" are metrics that need to be used as well...	In	D	Yes	Review	The hypothesis concerning ISP motivations for colocating speed test servers and a supposed relationship to video streamers is incorrect (speaking as an ISP engineer who personally made this decision for my company and encouraged other ISPs to do the same). This colocation occured many years prior to the rise of any video streaming and was intended to help accurately measure access network performance by eliminating often very low quality & over-capacity SFI links that were used in that era. In addition, it was done when the most popular test site provided a private label speed test UI to ISP which necessarily involved colocation of servers to support this, which could also be used by that test provider's own branded test.	Omer: definitely one of the more controversial positions so far. Putting the question of motivations aside (there are many ISPs, and it's plausible that the ones that Keith had experience with are not the same ones that Jason had experience with), it raises a bigger question, which is directly relevant to the workshop:  Are there different communication strategies when communicating network quality to ISPs vs. vendors vs. end users?   This topic is quite relevant to the workshop, and I believe that this discussion will contribute to the workshop.  Because of that, I am including this contribution in.	Metrics	Metrics
25	The Internet is a Shared Network	Stuart Cheshire	cheshire=40apple.com@dmarc.ietf.org			8/16/2021	Jason Jari	Include - strong paper & clear point of view that will likely provoke interesting discussion.	In	Day 1 : 14:10	yes	Yes			Problems	Intro
26	Beyond Speed Test: Measuring Latency Under Load Across Different Speed Tiers	Kyle MacMillian, Nick Feamster	macmillan@uchicago.edu, feamster@uchicago.edu	Edge-based measurement, latency measurement, latency-under-load (LUL), experiment, isp eval		8/16/2021	Kathie	Accept if room - In its favor, this is a measurement study instrumenting real devices and the data is well-presented Less great - the position is that latency under load is important to the user but the measurements are completely artificial traffic and there are no insights into what causes the differences. A reason to include this would be to perhaps discuss how "latency under load" measurements could be improved in their future work perhaps resulting in an easy to deploy device (they used rasPi) that could be used by a widely distributed group of users		Day 2	Yes	Review			Metric measurement	Metrics
27	10 Years of Internet-QoE Measurements. Video, Clould, Conferencing, Web and Apps. What do we Need from the Network Side?	Pedro Casas	Pedro.Casas@ait.ac.at	QoE, broad, introduction?		8/16/2021	Greg, Olivier	Include - the author provides a long bibliography of published work (36 papers) that he and his team have written on Internet-QoE over the past 10 years.  While I've not read the papers themselves, it appears that he has a lot of very relevant knowledge for the workshop. Olivier: The interesting point of the paper is the broad view on QoE. They have developed several models and tools and the lessons learned from them would be useful at the workshop. Also, the mention of user studies is relevant an useful.	In	Day 1	Yes	Sent			General intro	Intro
28	Fine-Grained RTT Monitoring Inside the Network	Satadal Segupta, Hyojoon Kim, Jennifer Rexford	satadals@cs.princeton.edu, hyojoonk@cs.princeton.edu, jrex@cs.princeton.edu	Passive measurement, in-network monitoring, experiment, solution		8/16/2021	Kathie Geoff	Geoff: I support inclusion. The introduction of high speed P4 chips in the network infrastructure gives some new insights into the behaviours of individual sessions within highly aggregated network circuits and the internal behaviour of network buffers within a switching element. It exposes some interesting questions such as the impact of "microbursts" on flows and a better understanding of buffer size and flow behaviour.       Kathie: Accept if room. A reason to include this would be to stimulate discussion on how in-network passive measurement truly could be useful and how such data ought to be collected and the authors’s work toward efficiency. OTOH, this doesn’t seem to represent a position on QoE but covers some implementation details of a passive in-network monitoring device to compute RTTs. As such, it’s not clear how different this is from related work on passive measurement of RTTs, including those on the reference list (e.g., Ref 11 talks about determining the validity of the paired packets for measurement). My open source tool takes a slightly different approach to RTT (https://github.com/pollere/pping) but all approaches have a lot of commonality and deal with similar issues of noisy data. There is the assertion that these in-network RTTs represent a QoE metric (no real argument, but the correlation to QoE depends on the application really and is not discussed) but the discussion of how it might be used, specifically noting that a “steadily increasing RTT” would indicate a decline in QoE to which the network could “adapt” in some unspecified manner shows a lack of understanding about what normally causes this. As someone who has spent some time looking at traffic and talking to others who do so, this is a pretty typical behavior of starting something like a video and experiencing bufferbloat which is unlikely to be solved by switching to another route. The timescales of increasing delay can also be important here. In my own observations (e.g. http://pollere.net/Pdfdocs/EdgeVideoIRTFMAP.pdf) video streams in particular show build ups of delay but long idle times. (The subsequent deployment of new congestion control methods and of AQM may have affected all of this in a positive way.) Finally, I’m wondering what a focus on TCP measurement might mean as QUIC becomes more widely deployed. 		Day 3	Yes	Sent			Metric measurements	Metrics
29	User-Perceived Latency to measure CCAs	Mingrui Zhang, Vidhi Goel, Lisong Xu	mzhang23@huskers.unl.edu, vidhi_goel=40apple.com@dmarc.ietf.org, xu@unl.edu	measurement		8/16/2021	Jason, Mirja	Include - while the proposal is fairly high level it attemps to define a standard for measuring/comparing working latency. This seems like it will be a useful part of discussion concerning how to measure they key user QoE factors. Mirja: Slightly out of scope. This paper proposed to optimize congstion control by reducing sending buffer delay (e.g. when the transmission is congestion-limited) as buffer delays are also one component of user-perceived latency (UPL). First, I don't thiink reducing sending delay is feasible because there will always be buffering when more data is ready to send that the link capacity can handle. Second I think design of congestion control is out of scope for this workshop.	In	Day 3	Yes	Sent			Solutions. The ideas intersects with other proposals	Solutions
30	Latency Measurement: What is latency and how do we measure it?	Karthik Sundaresan, Greg White, Steve Glennon 	k.sundaresan@cablelabs.com, g.white@cablelabs.com, s.glennon@cablelabs.com			8/30/2021	Jari, Geoff, Mirja	Geoff: I'm doubtful about the aproach proposed here, wherre the authors argue that a true latency measure should include host buffering components of the response. They argue this from the perspective of "responsiveness", but if you truly wanted to measure clieck to responsiveness than you need to include the DNS, the setup of TCP and then TLS (or QUIC) then HTTP startup. The point is that once you move from a network centric latency measurement to an application centric responsivess measure then you need to drag the protocol behaviours into consideration and host buffering is really just a minor part of application responsiveness. Mirja: Maybe invite authors but no need to republish. This paper has been published as a report here: https://www.nctatechnicalpapers.com/Paper/2020/2020-latency-measurement So probably there is no need to republish it on the IAB page rather than providing just a link to the paper. Also, the report is mainly only focused on summarizing existing techniques and metrics around latency. While there is a meta argument that latency as metric can easily be misinterpreted (e.g but looking only at averages but not the peaks), the actually "position"/new proposals in this report are very limited: there might be a few points in the conclusion, like using the 99 percentile to ensure coonsitent (low) latency or being able to distinguish latencies in different parts of the network. These are interesting points to discuss at the workshop (even though a focus on latency only might be a bit llimiting), however, a separate position paper raising these points would have been better as a basis for discussion IMO. Jari: Possible invite. I agree with Mirja and Geoff in what they say above. I do think this paper/document is a good treatment of overall issues around latency measurements, but at the same time thin on new proposals (that may be fine!) and doesn't for instance discuss QUIC measurements or other potentially new aspects arising from tecchnology development.	Out			Review				
31	End Users are not single-homed anymore	Olivier Bonaventure, Nicolas Keukeleire	olivier.bonaventure@tessares.net, nicolas.keukeleire@tessares.net			8/30/2021	Geoff	Geoff: Suggest Not. I'm not all that enthusiastic about this paper. Its true that there are many instsances of multiple paths in client-to-server connections, particularly when dual stack scenarios are considered, butg the ability of transport protocols and applications to make simultaneuous use of diverse paths is not in evidence in today's environemnt (even Multipath TCP is used in a A/B switch mode) so the arguments made in the paper that this area merits further study and development of measurement techniques in terms of end user quality of service is prtetty tenuous. 	Out			Review				
32	Responsiveness under Working Conditions    camera-ready: https://drive.google.com/file/d/1zh7b25n8ePvRE4aSzPQ138NcOpZveqSX/view?usp=sharing	Christoph Paasch, Randall Meyer, Stuart Cheshire, Omer Shapira	cpaasch@apple.com, rmm@apple.com, cheshire@apple.com, oesh@apple.com			8/30/2021	Kathie, Sam	Kathie: Invite. This brings up many important issues, but I think the work also has many common flawed assumptions that I’d really like to see discussed. Will put this in “comments to authors”. To authors: [see column to the right]	In	Day 3		Sent	This is a nice early step in thinking about user-facing metrics. The work sets out to measure “Responsiveness under Working Conditions” and then equates all delays in responsiveness to bufferbloat. Though the document nicely identifies that there are many sources of bb, I believe that some wifi and os issues may not actually be bufferbloat, but it is still important to measure and identify the sources of these delays. Further, the statement is “the focus of our responsiveness metric is to evaluate a real user-experience” but I don’t think that is what is happening with this Round Trips per minute metric. I have two major quibbles with the metric. First, the authors say “measuring bufferbloat requires us to fill the buffers of the bottleneck and when buffer occupancy is at its peak, the latency measurement needs to be done.” and in section 4.1.3 talk about achieving a goal of “saturation” by limiting themselves to loss-based cc. This is not bb, but *potential* bufferbloat and is not consistent with the goal of measuring “working conditions” or what a user actually experiences. Further, all the construction of a load to measure ends up reflecting the measurers’ biases. (And I am reflecting my own biases that passive measurement is where its at.) So a big problem with active measurement is represented by 4.1 which notes how important it is to create “typical working conditions” - this means the measurement will always reflect whatever the measurer thought were “typical working conditions” which may be incorrect and which will most certainly evolve. Although I agree with the authors’ that a simple presentation of results is needed, I am not certain that it has to be reduced to a single number. I am also disappointed to see the use of averages rather than more internet-traffic appropriate order stats. Finally, in 4.4.2 Statistical Confidence “One could imagine”? But this isn’t being done? Is it a work item? (Oh, extend <= extent)			solution
33	The 2021 National Internet Segment Reliability Research	Alexander Kozlov 	ak@qrator.net			8/30/2021	Geoff	Geoff: Suggest Not. I'm left questioning a) what this paper is about and b) what its relevance to the topic of measuring network quality for End-Users. The paper contains a large section about the state of IPv6 adoption, which seems to be a little off track. The desktop analysis of the robustness of the inteer-AS topology does not seem to be well-grounded, nor its relevance to the end-user experience, particularly with respect to the current levels of adoption of CDN-based content distribution for end-user service delivery.	Out			Review				
34	Packet delivery time as a tie-breaker for assessing Wi-Fi access points	Olivier Bonaventure, Francois Michel	olivier.bonaventure@uclouvain.be, francois.michel@uclouvain.be	wireless, local hop latency		8/30/2021	Toke, Jason	Toke: Suggest to include. The paper suggests exposig a relatively simple metric to applications, namely the average delivery time of WiFi frames at the MAC layer, which can serve as an additional metric of a WiFi network's quality. While this is not a revolutionary internet architecture idea, it seems like imminently practical to implement, and by simply providing more information to applications (or users) will allow those to make their own decisions. Jason: Suggest to include. Agree with Toke to include.	In	Day 2		Sent				wireless
35	Non-traditional Network Metrics	Eve M. Schooler, Rick Taylor	eve.m.schooler@intel.com, rick@tropicalstormsoftware.com			8/30/2021	Toke, Jason	Toke: Maybe. The paper talks about the need for alternative metrics like energy consumption or privacy to network paths. While I think it is interesting to consider such metrics, I am not sure how this would fit into the overall workshop; hence the 'maybe'. Olivier : I'm more positive than Toke. Besides performance, it can be useful to consider other metrics. The position paper argues well for energy consumption but is more convincing for the privacy/anonymity part. It is very likely that energy consumption/carbon footprint will become a key differentiator for some Internet users and having the ability to measure those aspects in a real deployment. Maybe this would be difficult to do during the workshop, but if the paper is not selected, we should at least encourage the authors to pursue the work. Jason: Suggest not. IMO there is sufficient challenge just standardizing 'traditional' metrics before we are mature enough to expand to the non-traditional ones like carbon intensity.	Out			Review				
36	Internet's performance from Facebook's edge	Brandon Schlinker	bschlinker@inbound5.com	large-scale QoE in diverse real-world scenarios		8/30/2021	Geoff	Geoff: Suggest IN. The paper provides a rare insight into the efforts to engineer service quality to end users in one of the larger content delivery networks in the Internet today. The observations relating to the use of selection of service delivery points to minimise the RTT to the end user are important observations, as is the observation of the prevalence of high capacity goodput in many access networks. The measurement of session quality comparison between peer and transit paths to the end user are informative, particularly noting the peer paths tend to offer lower RTTs.	In	Day 1		Sent				Metrics
37	network-quality-eckert-clemm-00.4	Toerless Eckert, Alex Clemm	tte@cs.fau.de, alex@clemm.org			8/30/2021	Mirja	Mirja: This paper discusses measurement and service quality from the network point of view. They also proposed latency based routing or deterministc network which however seems out of scope for this workshop.For the authors: Section 3.1 claims that only 10% of the traffic goes "over the Internet" -> would be good to provide a reference for these kind of claims! 	Out			Review				
38	Measuring newtork quality - the MONROE experience	Anna Brunstrom	anna.brunstrom@kau.se			8/30/2021	Olivier, Mirja	Olivier: The paper provides mainly two types of pointers to work carried on within the MONROE project: (1) work on transport services related to the TAPS working group and (2) experiments carried within the MONROE testbed. This testbed was a large testbed that combines mobile networks. Mobile networks are an important part of the user experience theses days and it is important to get the lessons obtained from these testbeds. Testbeds are a good platform to develop new measurement tool to assess the different metrics that characterize the performance of networks.  Mirja: I worked with the Monroe project and this is a really nice measnurement platform. The paper only described their platform and not what the realtion or interesting in the workshop topic is. While the platform is more directed towards researchers for experimentation and not at alll focused on user-consumable metric, the project still might have some interesting insight about useful metric more generally.	Out			Review				
39	Lower layer performance not indicative of upper layer success.  camera read: https://drive.google.com/file/d/1IFaBafQs4RVH-FA8jvQdgsvijvx-lQsy/view?usp=sharing	Lucas Pardue, Sreeni Tellakula	lucas=40cloudflare.com@dmarc.ietf.org, sreeni@cloudflare.com			8/30/2021	Olivier, Mirja	Olivier: Interesting position paper that explains in clear terms several important concepts and focuses on user relevant metrics. I would personnally have preferred to see a bibliography with references in the paper, but that's my academic bias.. Mirja: This is a really interesting read stating a clear position that is relevant for the workshop to discuss: Providing more metrisc to end user just gives an illusion of choice given the complexity of measurement of both the network as well as the application performance. 	in	Day 1		Sent				
40	An end-user approach to the Internet Score    camera-ready: https://drive.google.com/file/d/1Wq0_39UaIDCLTV3sw2DnpSDePDLDx8lq/view?usp=sharing	Christoph Paasch, Kristen McIntyre, Randall Meyer, Stuart Cheshire, Omer Shapira	cpaasch@apple.com, kristen@apple.com, rmm@apple.com, cheshire@apple.com, oesh@apple.com			8/30/2021	Mirja, Jason	Mirja: This paper proposed a new dimentionless, combined metric called Internet Score. This is fully in scope for the workshop and could probably foster some really interesting discussions! Jason: Agree - recommend including presentation. Usually we think of metrics as being sort of one-dimensional. This presents the suggestion for a measurement comprised of several components and that those components may be updated over time - sort of like the Dow Jones Industrial Average. This is a novel approach. 	in	Day 3		Sent				metrics
41	A single common metric to characterize varying packet delay	Bob Briscoe, Greg White, Vidhi Goel and Koen De Schepper	ietf@bobbriscoe.net, g.white@CableLabs.com, vidhi_goel@apple.com, koen.de_schepper@nokia.com	latancy		8/30/2021	Olivier, Mirja	Olivier: This paper argues for using percentile 99 to characterize delay variation and that a single percentile would be easier to understand (although difficult) by endusers. This looks an interesting direction whcih could warrant interesting discussion. I don't think that the delay variation can be specified without mentioning load. A flow of 1 Kbps might not suffer from the same delay variation as a flow at 1 Mbps or 100 Mbps. In this that the type of flow used for the measurements should be clarified in the final version of the paper.            Mirja: This paper proposed a single industry-wide percentile to characterize delay and provides some background how to decide about the used percentile and which this metric is sufficient/better than average delay and jitter. This is in scope and together with other paper that discuss use of high percentiles for delay good input for the workshop.	in	Day 3		Sent				metrics
42	Regulartory perspective on measuring network quality for the end users (abstract)   Full presentation	Ahmed Aldabbagh	Ahmed.Aldabbagh=40ofcom.org.uk@dmarc.ietf.org			8/30/2021	Olivier, Mirja, Jason	Mirja: This paper very briefly proposes a presentation about the regulator's perpective on metric. There are not many details given in the paper what the imporatnt points about this perspective are. HHowever, having the regulator's perspective, to some extend also as a proxy for user rights, in this workshop is for sure intereting. Recommend to invite but maybe no real need to publish thiss paper - or ask for revission that you actually contain the content of the proposed presentation rather than only a high level outline.  Olivier: I agree with Mirja, interesting to have regulators in the workshop, but the content of the current paper is not sufficient to warrant a publication. Jason: Agree that the paper is not compelling per se, but this person's perspective as a regulator would be quite interesting. Recommend inviting but no presentation.	Out			Sent				
	Submissions below have been received after the Aug 2nd deadline and need to be reviewed by Aug 30th.															
	The submission deadline has been closed. Any additional submissions / invitations are left to the workshop chairs decision.															
